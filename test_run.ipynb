{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import spacy\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "# Set the NLTK data directory\n",
    "nltk.data.path.append(os.getenv('NLTK_DATA', '/nltk_data'))\n",
    "\n",
    "# # Load spaCy model from the specified directory\n",
    "# nlp = spacy.load(os.getenv('SPACY_DATA') + '/en_core_web_md')\n",
    "\n",
    "\n",
    "# model_path = \"./model\"\n",
    "# model = SentenceTransformer(model_path, cache_folder='/tmp/sentence_transformers_cache')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    '''Provide an event that contains the following keys:\n",
    "\n",
    "      - message: contains the text \n",
    "    '''\n",
    "    try:\n",
    "        inputText = event['questionInfo']\n",
    "        lowerBound, upperBound = prediction(inputText)\n",
    "\n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({\"upperBound\" : upperBound,\n",
    "                                \"lowerBound\": lowerBound})\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"statusCode\": 500,\n",
    "            \"body\": json.dumps({\"error\": repr(e)})\n",
    "        } \n",
    "\n",
    "\n",
    "\n",
    "## to process y\n",
    "# clean up the formatting of questions, and break them into sentences\n",
    "def clean_text(text):\n",
    "\n",
    "    modified_text = re.sub(r'^o |\\\\n|\\n|&#x20;|\\s+|(?<=i\\.e)\\.|(?<=e\\.g)\\.', ' ', text)\n",
    "    modified_text = re.sub(r'\\$\\$', r'$', modified_text, flags=re.DOTALL)\n",
    "    modified_text = re.sub(r'!?\\[[^\\]]*\\]\\([^\\)]+\\)', ' image ', modified_text)\n",
    "    modified_text = re.sub(r'\\|.*\\||\\<.*?\\>|```[\\s\\S]*?```', '[embedded]', modified_text)\n",
    "    return modified_text\n",
    "\n",
    "def clean_latex(text):\n",
    "   \n",
    "    text = clean_text(text)    \n",
    "    return text\n",
    "\n",
    "\n",
    "def clean_and_restore_latex(text):\n",
    "    text = clean_text(text)\n",
    "    latex_segments = re.findall(r'\\$ .*? *\\$|\\$.*?\\$', text, flags=re.DOTALL)\n",
    "    for i, pattern in enumerate(latex_segments):\n",
    "        text = text.replace(pattern, f' latex{i} ')\n",
    "    text = re.sub(r'[\\]\\[*()\\\\]', '', text)\n",
    "    for i, pattern in enumerate(latex_segments):\n",
    "        pattern = pattern.replace('.',',')\n",
    "        text = text.replace(f'latex{i}', pattern)\n",
    "    return text\n",
    "\n",
    "# to distill questions from the entire problem statement\n",
    "# List of target verbs to check within sentences\n",
    "target_verbs = {\n",
    "    \"add\", \"adjust\", \"advise\", \"analyse\", \"apply\", \"approximate\", \"arrange\", \n",
    "    \"build\", \"calculate\", \"change\", \"check\", \"choose\", \"combine\", \"comment\", \"complete\",\n",
    "    \"compute\", \"consider\", \"construct\", \"convert\", \"count\", \"create\", \"deduce\", \"define\",\n",
    "    \"demonstrate\", \"derive\", \"describe\", \"determine\", \"develop\", \"differentiate\", \"dimensionalise\",\n",
    "    \"distinguish\", \"divide\", \"draw\",\"discuss\", \"eliminate\", \"enter\", \"estimate\", \"evaluate\", \"expand\",\n",
    "    \"explain\", \"explore\", \"express\", \"extract\", \"fill\", \"find\", \"follow\", \"formulate\",\n",
    "    \"generate\", \"give\", \"how\", \"identify\", \"illustrate\", \"implement\", \"improve\", \"include\",\n",
    "    \"infer\", \"insert\", \"integrate\", \"introduce\", \"investigate\", \"justify\", \"label\", \"make\",\n",
    "    \"mark\", \"maximize\", \"measure\", \"minimize\", \"modify\", \"multiply\", \"mutate\", \"normalise\",\n",
    "    \"obtain\", \"perform\", \"plot\", \"predict\", \"propose\", \"provide\",\n",
    "    \"rearrange\", \"recalculate\", \"reduce\", \"remove\", \"replace\", \"replicate\", \"reproduce\",\n",
    "    \"retain\", \"rewrite\", \"select\", \"separate\", \"share\", \"show\", \"simplify\", \"sketch\",\n",
    "    \"solve\", \"specify\", \"state\", \"submit\", \"subtract\", \"sum\", \"tabulate\", \"transform\",\n",
    "    \"translate\", \"try\", \"update\", \"use\", \"verify\", \"what\", \"when\", \"which\", \"who\", \"why\", \"write\"\n",
    "}\n",
    "\n",
    "\n",
    "# Function to detect if a sentence is a question based on given criteria\n",
    "def is_question(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    # Check if the sentence ends with a question mark\n",
    "    if sentence.strip().endswith(\"?\"):\n",
    "        return True\n",
    "    # Check if the sentence starts with a verb\n",
    "    if doc[0].pos_ == \"VERB\":\n",
    "        return True\n",
    "    \n",
    "    question_words = [\"who\", \"what\", \"when\", \"where\", \"why\", \"how\"]\n",
    "    first_word = doc[0].text.lower()\n",
    "    if first_word in question_words:\n",
    "        return True\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in target_verbs and token.pos_ == \"VERB\" and token.dep_ == \"ROOT\" and (token.tag_ == \"VB\" or token.tag_ == 'VBP'):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def lenient_is_question(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    for token in doc:\n",
    "        if token.lemma_.lower() in target_verbs:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_term_title(text, terms):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "\n",
    "    for term in terms:\n",
    "        term_tokens = term.split()\n",
    "        term_length = len(term_tokens)\n",
    "        for i in range(len(doc) - term_length + 1):\n",
    "            if all(doc[i+j].lemma_.lower() == term_tokens[j] for j in range(term_length)):\n",
    "                tokens.append(term)\n",
    "                break\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_term_solution(text, terms):\n",
    "    doc = nlp(text)\n",
    "    tokens = []\n",
    "    tokens = [j.lemma_.lower()  for j in doc if j.lemma_.lower() in terms]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def extract_question_verbs(text):\n",
    "    fivew1h_tags = {'WP', 'WP$', 'WRB', 'WDT'}\n",
    "    doc = nlp(text)\n",
    "    # tokens = [token.lemma_ for token in doc if token.text.lower() in ['fourier','series']]\n",
    "    tokens = [token.lemma_ for token in doc if token.pos_ in ('VERB') and token.dep_ == \"ROOT\" and token.tag_ in (\"VBP\",\"VB\")] + [token.text for token in doc if token.tag_ in fivew1h_tags]\n",
    "    # tokens =  [token.text for token in doc if token.tag_ in fivew1h_tags]\n",
    "\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# to clean and tokenize the text\n",
    "def custom_tokenize(text):\n",
    "    \n",
    "    modified_text = re.sub(r'\\\\\\$', '', text)\n",
    "    modified_text = re.sub(r'\\n', ' ', text)\n",
    "    latex_segments = re.findall(r'\\$\\$.*?\\$\\$|\\$.*?\\$', modified_text, re.DOTALL)\n",
    "    modified_text = re.sub(r'!?\\[[^\\]]*\\]\\([^\\)]+\\)|\\|.*\\||\\<.*?\\>|```[\\s\\S]*?```', '[embedded]', modified_text)\n",
    "    phrases = re.split(r'(\\$\\$[\\s\\S]*?\\$\\$|\\$.*?\\$)', modified_text)\n",
    "    latex_tokens, tokens = [], []\n",
    "    for phrase in phrases:\n",
    "        if phrase in latex_segments:\n",
    "            tokens.append(\"[latex]\")  \n",
    "            latex_tokens.append(phrase)\n",
    "        else:\n",
    "            # tokens.extend(nltk.word_tokenize(phrase))  \n",
    "            tokens.extend(phrase.split(' '))  \n",
    "    return tokens, latex_tokens\n",
    "\n",
    "# count the number of various operations\n",
    "def element_counter(text):\n",
    "    operator = r'\\\\approx|\\\\parallel|\\\\nonumber|\\\\cdot|[<>+-=~]|\\\\times|\\\\div|\\\\pm|\\\\sum|\\\\sqrt|\\\\root||\\\\equiv|\\\\ne|\\\\neq|\\\\leq|\\\\le|\\\\limit|\\\\in|\\\\implies|\\\\gg|\\\\geq|\\\\ge|\\\\ll'\n",
    "\n",
    "    num_operator = len(re.findall(operator,text))\n",
    "    text = re.sub(r'\\s+','',text)\n",
    "    return num_operator, text\n",
    "\n",
    "def transform(self, data):\n",
    "    if not self.fitted:\n",
    "        raise ValueError(\"The label encoder has not been fitted yet.\")\n",
    "    \n",
    "    unique_labels = set(self.label_encoder.classes_)\n",
    "    transformed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        if item in unique_labels:\n",
    "            transformed_data.append(self.label_encoder.transform([item])[0])\n",
    "        else:\n",
    "            transformed_data.append(-1)  # Default value for unseen labels\n",
    "    \n",
    "    return np.array(transformed_data)\n",
    "\n",
    "# to use for test set to encode\n",
    "def prediction(df):\n",
    "\n",
    "    # df[\"module\"] = re.match(r'^[A-Z]{3,}', df[\"modulename\"].split(' ')[0]).group() if re.match(r'^[A-Z]{3,}', df[\"modulename\"].split(' ')[0]) else 'OTHERS'\n",
    "    df[\"level\"] =  re.match(r'^[A-Z]+(\\d)', df[\"modulename\"].split(' ')[0]).group(1) if re.match(r'^[A-Z]+(\\d)', df[\"modulename\"].split(' ')[0]) else '4'\n",
    "\n",
    "    # with open('le_module.pkl', 'rb') as file:\n",
    "    #     le_module = pickle.load(file)\n",
    "    # if df[\"module\"] not in le_module.classes_:\n",
    "    #     df[\"module\"] = 0\n",
    "    # else:\n",
    "    #     df[\"module\"] = le_module.transform([df[\"module\"]])[0]\n",
    "\n",
    "    print(\"Start\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    with open('le_level.pkl', 'rb') as file:\n",
    "        le_level = pickle.load(file)\n",
    "    if df[\"level\"] not in le_level.classes_:\n",
    "        df[\"level\"] = '4'\n",
    "    else:\n",
    "        df[\"level\"] = le_level.transform([df[\"level\"]])[0]\n",
    "\n",
    "    # concat all the question text\n",
    "    df[\"total_text\"] = df[\"masterContent\"] + ' ' + df[\"partContent\"]\n",
    "\n",
    "    # impute partContent with masterContent if completely empty\n",
    "    # pattern = r\"^(\\n)*$\"\n",
    "    # df['partContent'] = df['masterContent'] if re.match(pattern, str(df['partContent'])) else df['partContent']\n",
    "\n",
    "    # to distill questions from the entire problem statement\n",
    "\n",
    "    ans = []\n",
    "    print(\"before clean_and_restore\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    cleaned_text = clean_and_restore_latex(str(df[\"total_text\"]))\n",
    "    # sentences = [sent for text in cleaned_text for sent in nltk.sent_tokenize(str(text))]\n",
    "    print(\"after clean_and_restore\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    questions = nltk.sent_tokenize(cleaned_text)\n",
    "    print(\"after tokenize\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    # Detect questions in the list of sentences\n",
    "    ans = [sentence for sentence in questions if is_question(sentence)]\n",
    "\n",
    "    if ans == []:\n",
    "         ans = [sentence for sentence in questions if lenient_is_question(sentence)] \n",
    "\n",
    "    print(\"after isQuestion\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    # add the distilled questions back to the main table\n",
    "    df[\"questionContent\"] = ans\n",
    "    # df[\"question_sentence_len\"] = len(ans)\n",
    "\n",
    "\n",
    "    #impute questionContent with partContent if not found.\n",
    "    df['questionContent'] = [df['partContent']] if df[\"questionContent\"] == [] else df['questionContent']\n",
    "\n",
    "    print(\"before title vectornizer\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    # extract noun from title\n",
    "    with open('title_noun_vectorizer.pkl', 'rb') as file:\n",
    "        title_noun_vectorizer = pickle.load(file)\n",
    "    # identified top20 terms with stronger correlation with label i.e >0.08 correlation and appeared\n",
    "    # at least 5 times\n",
    "    title_noun_terms = [\"challenge\",\"hydrostatic\",\"tunnel\", \"strain\",\"wind\",\"cycle calculations\",\"fourier\",\"flow\",\"fluid\",\"stresses\"]\n",
    "\n",
    "    tokenized_doc = [extract_term_title(df['questiontitle'], title_noun_terms)]\n",
    "    count_matrix = title_noun_vectorizer.transform(tokenized_doc)\n",
    "\n",
    "    X_title_noun = count_matrix.toarray()\n",
    "\n",
    "    print(\"after title vectornizerm  before sol vetornize\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    # extract verb, noun from solution\n",
    "\n",
    "    with open('q_vectorizer.pkl', 'rb') as file:\n",
    "        q_vectorizer = pickle.load(file)\n",
    "\n",
    "    # sol_terms = [\"image\",\"equation\",\"use\",\"substitute\",\"follow\",\"apply\",\"part\",\"find\",\"note\",\"give\",\"rearrange\",\"diagram\",\"look\",\"stress\"]\n",
    "    # q_terms =[ 'delta', 'engine', 'find', 'frac', 'infty', 'left', 'omega', 'part', 'pressure', 'prime', 'river', 'shaft', 'show', 'text']\n",
    "    q_terms =[ 'delta', 'engine', 'find', 'frac', 'infty', 'left', 'omega', 'part', 'pressure', 'prime', 'shaft', 'show', 'text']\n",
    "\n",
    "\n",
    "\n",
    "    # Apply the extraction function to the DataFrame\n",
    "    tokenized_doc = [extract_term_solution(str(df['total_text']), q_terms)]\n",
    "\n",
    "    count_matrix = q_vectorizer.transform(tokenized_doc)\n",
    "\n",
    "    X_q_noun = count_matrix.toarray()\n",
    "\n",
    "\n",
    "\n",
    "    with open('sol_vectorizer.pkl', 'rb') as file:\n",
    "        sol_vectorizer = pickle.load(file)\n",
    "\n",
    "    sol_terms = ['align', 'array', 'cfrac', 'dfrac', 'dot', 'e_', 'frac', 'lambda', 'ldots', 'left', 'mathrm', 'omega', 'partial', 'ratio', 'right', 'sigma_', 'text', 'theta']\n",
    "\n",
    "\n",
    "    # Apply the extraction function to the DataFrame\n",
    "    tokenized_doc = [extract_term_solution(str(df['workedsolution']), sol_terms)]\n",
    "\n",
    "    count_matrix = sol_vectorizer.transform(tokenized_doc)\n",
    "\n",
    "    X_sol_noun = count_matrix.toarray()\n",
    "    # to extract all question verb \n",
    "    print(\"after sol vectoriznt\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "    # Extract nouns and verbs from the text data\n",
    "    tokenized_doc = extract_question_verbs(str(df[\"questionContent\"]))\n",
    "    # processed_docs = [doc for doc in  df[\"questionContent\"]]\n",
    "\n",
    "    verb_count = len(tokenized_doc.split())\n",
    "    df[\"verb_count_q\"] = verb_count\n",
    "\n",
    "    print(\"after verb count q, before token len count\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    tokenized_texts = custom_tokenize(str(df['total_text']))[0]\n",
    "    tokenized_texts = [i for i in tokenized_texts if i!='']\n",
    "    latex_terms = custom_tokenize(str(df['total_text']))[1]\n",
    "    len_text_text = len(tokenized_texts)\n",
    "    a =  element_counter(''.join(latex_terms))\n",
    "    len_latex_text = len(a[1])\n",
    "    df[\"text_latex_stats\"] = a[0]\n",
    "\n",
    "    # tokenized_solution = custom_tokenize(str(df['workedsolution']))[0]\n",
    "    # tokenized_solution = [i for i in tokenized_solution if i!='']\n",
    "    latex_terms_solution = custom_tokenize(str(df['workedsolution']))[1]\n",
    "    # len_text_solution = len(tokenized_solution)\n",
    "    a = element_counter(''.join(latex_terms_solution))\n",
    "    len_latex_solution = len(a[1])\n",
    "    df[\"solution_latex_stats\"]  = a[0]\n",
    "\n",
    "    tokenized_question = custom_tokenize(str(df['questionContent']))[0]\n",
    "    tokenized_question = [i for i in tokenized_question if i!='']\n",
    "    latex_terms_question = custom_tokenize(str(df['questionContent']))[1]\n",
    "    len_text_question = len(tokenized_question)\n",
    "    a = element_counter(''.join(latex_terms_question))\n",
    "    len_latex_question = len(a[1])\n",
    "\n",
    "    df[\"text_len\"] = len_text_text\n",
    "    df[\"latex_len\"] = len_latex_text\n",
    "    # df[\"latex_len_solution\"] = len_latex_solution\n",
    "    # df[\"text_len_solution\"] = len_text_solution\n",
    "    df[\"text_len_question\"] = len_text_question\n",
    "    df[\"latex_len_question\"] = len_latex_question\n",
    "\n",
    "    print(\"after token len count\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "\n",
    "    ## add new features \n",
    "    df[\"char_len_total_text\"] = len(str(df[\"total_text\"]).replace(' ','').replace('\\n',''))\n",
    "    df[\"total_sol_len\"] = len((str( df[\"workedsolution\"])).replace(' ','').replace('\\n',''))\n",
    "    df[\"steps\"] = str(df[\"workedsolution\"]).count(\"***\")\n",
    "    if df[\"steps\"] == 0 and df[\"total_sol_len\"] != 0:\n",
    "        # Count the number of colons\n",
    "        steps = df[\"workedsolution\"].count(\":\")\n",
    "        # If still zero, count the number of double newlines\n",
    "        if steps == 0:\n",
    "            steps = len(re.findall(r'(\\n\\n)+', str(df[\"workedsolution\"]))) // 2\n",
    "        # If still zero and 'workedsolution' is not empty, set steps to 1\n",
    "        if steps == 0:\n",
    "            steps = 1\n",
    "\n",
    "    df[\"skill_x_total_sol_len\"] = df[\"skill\"] * df[\"total_sol_len\"] \n",
    "    # df[\"skill_x_steps\"] = df[\"skill\"] * df[\"steps\"]\n",
    "    # df[\"level_x_total_sol_len\"] = df[\"total_sol_len\"] * df[\"level\"]\n",
    "\n",
    "    # print(\"before embedding\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    \n",
    "    # embeddings = model.encode(df[\"questionContent\"])\n",
    "    # print(embeddings.shape)\n",
    "    # with open('knn_cluster.pkl', 'rb') as file:\n",
    "    #     kmeans = pickle.load(file)\n",
    "    # cluster_label = kmeans.predict(embeddings)\n",
    "    # df[\"cluster\"] = cluster_label[0]\n",
    "\n",
    "    print(\"after embedding\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    included_fields = ['setnumber', 'questionnumber', 'skill', 'level', 'verb_count_q', 'text_latex_stats', 'solution_latex_stats', 'text_len', 'latex_len', \n",
    "                       'text_len_question', 'latex_len_question', 'char_len_total_text', 'total_sol_len', 'steps', 'skill_x_total_sol_len']\n",
    "    # X_input = [X_sol_noun[0], X_title_noun[0]]\n",
    "    # X_input.extend([ df[key] for key in included_fields])\n",
    "    # X_input = np.array([X_sol_noun[0], X_title_noun[0]] + [ df[key] for key in included_fields])\n",
    "    # X_input = np.array(X_input)\n",
    "    print(df)\n",
    "\n",
    "    X_input = np.concatenate([X_q_noun, X_sol_noun, X_title_noun, np.array([ [df[key]] for key in included_fields]).T  ], axis=1)\n",
    "    print(X_input)\n",
    "    with open('rf_classifier.pkl', 'rb') as file:\n",
    "        rf_classifier = pickle.load(file)\n",
    "\n",
    "    print(\"after rf_classify\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    bucket = rf_classifier.predict(X_input)[0]\n",
    "    bucket = round(bucket)\n",
    "    lower = max(bucket - 8,0)\n",
    "    upper = bucket + 8\n",
    "    return lower, upper\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start 09:36:59\n",
      "before clean_and_restore 09:36:59\n",
      "after clean_and_restore 09:36:59\n",
      "after tokenize 09:36:59\n",
      "after isQuestion 09:36:59\n",
      "before title vectornizer 09:36:59\n",
      "after title vectornizerm  before sol vetornize 09:36:59\n",
      "after sol vectoriznt 09:36:59\n",
      "after verb count q, before token len count 09:36:59\n",
      "after token len count 09:36:59\n",
      "after embedding 09:36:59\n",
      "{'questionnumber': 5, 'modulename': 'module name', 'setnumber': 6, 'questiontitle': 'title', 'masterContent': 'content', 'partPosition': 3, 'workedsolution': 'solContent', 'partContent': 'partContent', 'skill': 1, 'level': 0, 'total_text': 'content partContent', 'questionContent': ['partContent'], 'verb_count_q': 0, 'text_latex_stats': 1, 'solution_latex_stats': 1, 'text_len': 2, 'latex_len': 0, 'text_len_question': 1, 'latex_len_question': 0, 'char_len_total_text': 18, 'total_sol_len': 10, 'steps': 0, 'skill_x_total_sol_len': 10}\n",
      "[[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "   0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  6  5  1  0  0  1  1\n",
      "   2  0  1  0 18 10  0 10]]\n",
      "after rf_classify 09:36:59\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 17)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "request = {\"questionnumber\": 5,\n",
    "    \"modulename\": \"module name\",\n",
    "    \"setnumber\": 6,\n",
    "    \"questiontitle\": \"title\",\n",
    "    \"masterContent\": \"content\",\n",
    "    \"partPosition\": 3,\n",
    "    \"workedsolution\": \"solContent\",\n",
    "    \"partContent\": \"partContent\",\n",
    "    \"skill\": 1}\n",
    "\n",
    "\n",
    "prediction(request)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
